/**
 * ResearchSpace
 * Copyright (C) 2020, Â© Trustees of the British Museum
 * Copyright (C) 2015-2019, metaphacts GmbH
 *
 * This program is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.

 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */

package org.researchspace.rest.endpoint;

import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.net.URLEncoder;
import java.time.Instant;
import java.util.Date;
import java.util.List;
import java.util.Optional;
import java.util.stream.Collectors;

import javax.inject.Inject;
import javax.ws.rs.Consumes;
import javax.ws.rs.DefaultValue;
import javax.ws.rs.POST;
import javax.ws.rs.Path;
import javax.ws.rs.QueryParam;
import javax.ws.rs.core.MediaType;
import javax.ws.rs.core.Response;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;
import org.apache.shiro.authz.annotation.Logical;
import org.apache.shiro.authz.annotation.RequiresAuthentication;
import org.apache.shiro.authz.annotation.RequiresPermissions;
import org.eclipse.rdf4j.common.iteration.Iterations;
import org.eclipse.rdf4j.model.IRI;
import org.eclipse.rdf4j.model.Model;
import org.eclipse.rdf4j.model.Statement;
import org.eclipse.rdf4j.model.ValueFactory;
import org.eclipse.rdf4j.model.impl.LinkedHashModel;
import org.eclipse.rdf4j.model.impl.SimpleValueFactory;
import org.eclipse.rdf4j.query.Update;
import org.eclipse.rdf4j.repository.RepositoryConnection;
import org.eclipse.rdf4j.repository.RepositoryException;
import org.eclipse.rdf4j.rio.RDFFormat;
import org.eclipse.rdf4j.rio.Rio;
import org.researchspace.api.sparql.SparqlOperationBuilder;
import org.researchspace.cache.CacheManager;
import org.researchspace.config.NamespaceRegistry;
import org.researchspace.data.rdf.PointedGraph;
import org.researchspace.repository.MpRepositoryProvider;
import org.researchspace.repository.RepositoryManager;
import org.researchspace.security.Permissions.FORMS_SPARQL;
import org.researchspace.security.SecurityService;
import org.researchspace.services.files.FileManager;
import org.researchspace.services.files.ManagedFileName;
import org.researchspace.services.storage.api.ObjectKind;
import org.researchspace.services.storage.api.ObjectMetadata;
import org.researchspace.services.storage.api.ObjectRecord;
import org.researchspace.services.storage.api.ObjectStorage;
import org.researchspace.services.storage.api.PlatformStorage;
import org.researchspace.services.storage.api.PlatformStorage.FindResult;
import org.researchspace.services.storage.api.StorageException;
import org.researchspace.services.storage.api.StoragePath;

/**
 * @author Johannes Trame <jt@metaphacts.com>
 *
 */
@Path("/sparql")
public class FormPersistenceSparqlEndpoint {
    private static final Logger logger = LogManager.getLogger(FormPersistenceSparqlEndpoint.class);

    @Inject
    private PlatformStorage platformStorage;

    @Inject
    private RepositoryManager repositoryManager;

    @Inject
    private CacheManager cacheManager;

    @Inject
    private NamespaceRegistry nsRegistry;

    private final ValueFactory vf = SimpleValueFactory.getInstance();
    
    @Inject
    private FileManager fileManager;

    /**
     * Executes and array of SPARQL INSERT and DELETE query strings in one
     * transaction. 
     * 
     * If targetInsertGraphIri or targetGraphIri (mutually exclusive) are passed down from a form submission, 
     * the corresponding graphs are saved in runtime storage; This is restricted to repositories: configurations and authorities
     * 
     * @param deleteAndInserts An JSON array of SPARQL INSERT and DELETE queries
     *                         generated by the client-side form component
     * @return
     */
    @POST
    @RequiresAuthentication
    @Consumes(MediaType.APPLICATION_JSON)
    @RequiresPermissions(value = { FORMS_SPARQL.CREATE, FORMS_SPARQL.UPDATE }, logical = Logical.OR)
    public Response executeUpdates(List<String> deleteAndInserts, @QueryParam("repository") String repositoryID, @QueryParam("graph") @DefaultValue("")  String graph) {
        if (repositoryID == null) {
            return Response.status(Response.Status.BAD_REQUEST).entity("Missing required parameter 'repository'")
                    .build();
        } 
        logger.debug("Received SPARQL insert and delete queries: {}", deleteAndInserts);
        
        try {
            try (RepositoryConnection con = repositoryManager.getRepository(repositoryID).getConnection()) {
                // TODO currently we can't execute all update operations in one transaction, see
                // https://github.com/eclipse/rdf4j/issues/972
                try {
                    for (String updateString : deleteAndInserts) {
                        Update update = SparqlOperationBuilder.<Update>create(updateString, Update.class)
                                .resolveUser(nsRegistry.getUserIRI()).build(con);
                        update.execute();
                    }
                    logger.info("updating"+repositoryID);
                    /* Special rule for the FieldCategories authority */
                    if (graph.equals("http://www.researchspace.org/resource/system/FieldCategories"))
                        /* specify a different repositoryId for saving the serialised data i.e. the ldp/authorities were all other authorities are */
                        repositoryID = "authorities";

                    if (graph != "" && (repositoryID.equals("configurations") || repositoryID.equals("authorities"))) {
                        IRI graphIri = vf.createIRI(graph);
                        Model model = new LinkedHashModel(Iterations.asList(con.getStatements(null,null, null,graphIri)));
                        PointedGraph pg = new PointedGraph(graphIri, model);
                        StoragePath objectId = ObjectKind.LDP.resolve(repositoryID)
                                                .resolve(StoragePath.encodeIri(pg.getPointer())).addExtension(".trig");
                        
                        ByteArrayOutputStream outStream = new ByteArrayOutputStream();
                        
                        List<Statement> toWrite = pg.getGraph().stream()
                                .map(stmt -> vf.createStatement(stmt.getSubject(), stmt.getPredicate(), stmt.getObject(), graphIri))
                                .collect(Collectors.toList());

                        if (toWrite.size()>0) {
                            Rio.write(toWrite, outStream, RDFFormat.TRIG);
                            byte[] bytes = outStream.toByteArray();
                             
                            ByteArrayInputStream content = new ByteArrayInputStream(bytes);
                            platformStorage.getStorage(PlatformStorage.DEVELOPMENT_RUNTIME_STORAGE_KEY).appendObject(objectId,
                            new ObjectMetadata(nsRegistry.getUserIRI().stringValue(),Instant.now()), content, bytes.length);  
                            logger.info("I should be saving here"+repositoryID);
                        } else {
                            try {
                                platformStorage.getStorage(PlatformStorage.DEVELOPMENT_RUNTIME_STORAGE_KEY).deleteObject(objectId,
                                        platformStorage.getDefaultMetadata());
                                // Check if the object still exists in some app (usually immutable)
                                Optional<FindResult> optObject = platformStorage.findObject(objectId);
                                if (optObject.isPresent()) {
                                    // We need to shadow the object to prevent loading from the app, so we save an
                                    // empty
                                    // model under the same name into the runtime storage
                                    //this.saveToStorage(new PointedGraph(graphIri, new LinkedHashModel()),
                                    //      graphIri);
                                }
                            } catch (StorageException e) {
                                throw new RepositoryException(
                                        "Could not delete the object " + objectId + " from storage: " + e.getMessage(), e);
                            }
                        }
                    }
                } catch (Exception e) {
                    // con.rollback();
                    // TODO it also means that we can't have rollbacks in case of error
                    throw e;
                }
              
            }

            // try to invalidate all caches i.e. forms are likely to have changed types or
            // labels
            // TODO remove once we have a global strategy for listening to
            // repository/resource changes
            try {
                cacheManager.invalidateAll();
            } catch (Exception e) {
                // we do not want the form transaction to fail only because for whatever reason
                // the invalidation has failed
                logger.error("Invalidation of caches failed: {}", e.getMessage());
            }
            return Response.ok().build();
        } catch (Exception e) {
            logger.error("Error while executing SPARQL updates for forms: {} ", e.getMessage());
            logger.debug("Details: {} ", e);
            return Response.serverError().entity(e.getMessage()).build();
        }
    }

}
